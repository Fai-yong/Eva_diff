#!/usr/bin/env python3
"""
Custom dataset loader for preference data generated by build_preference_dataset.py

This module provides utilities to load and use the preference dataset with
the DPO training script.
"""

import pickle
import json
from typing import List, Dict, Any
from torch.utils.data import Dataset


class PreferenceDataset(Dataset):
    """
    Custom dataset class for loading preference data.

    Compatible with the DPO training script's expected format.
    """

    def __init__(self, dataset_path: str):
        """
        Initialize the preference dataset.

        Args:
            dataset_path: Path to the .pkl file containing preference pairs
        """
        self.dataset_path = dataset_path

        print(f"Loading preference dataset from {dataset_path}")
        with open(dataset_path, 'rb') as f:
            self.data = pickle.load(f)

        print(f"Loaded {len(self.data)} preference pairs")

        # Validate data format
        self._validate_data()

    def _validate_data(self):
        """Validate that the data has the expected format."""
        if not self.data:
            raise ValueError("Dataset is empty")

        # Check first item has required fields
        sample = self.data[0]
        required_fields = ['jpg_0', 'jpg_1', 'label_0', 'caption']

        for field in required_fields:
            if field not in sample:
                raise ValueError(f"Missing required field: {field}")

        print("Dataset validation passed")

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Get a single preference pair.

        Returns:
            Dictionary with keys: jpg_0, jpg_1, label_0, caption, metadata
        """
        return self.data[idx]

    def get_metadata_summary(self) -> Dict[str, Any]:
        """Get summary statistics about the dataset."""
        if not self.data:
            return {}

        # Collect statistics
        primary_metrics = []
        score_differences = []

        for item in self.data:
            if 'metadata' in item:
                meta = item['metadata']
                if 'score_difference' in meta:
                    score_differences.append(meta['score_difference'])
                if 'primary_metric' in meta:
                    primary_metrics.append(meta['primary_metric'])

        summary = {
            'total_pairs': len(self.data),
            'primary_metric_used': primary_metrics[0] if primary_metrics else 'unknown',
            'score_difference_stats': {
                'mean': sum(score_differences) / len(score_differences) if score_differences else 0,
                'min': min(score_differences) if score_differences else 0,
                'max': max(score_differences) if score_differences else 0,
            } if score_differences else None
        }

        return summary


def convert_to_huggingface_format(dataset_path: str, output_path: str):
    """
    Convert preference dataset to HuggingFace datasets format.

    This creates a format that can be loaded directly by the training script
    using datasets.load_from_disk().

    Args:
        dataset_path: Path to the .pkl preference dataset
        output_path: Path to save HuggingFace dataset
    """
    from datasets import Dataset as HFDataset

    # Load preference data
    dataset = PreferenceDataset(dataset_path)

    # Convert to HuggingFace format
    hf_data = {
        'jpg_0': [],
        'jpg_1': [],
        'label_0': [],
        'caption': []
    }

    for item in dataset.data:
        hf_data['jpg_0'].append(item['jpg_0'])
        hf_data['jpg_1'].append(item['jpg_1'])
        hf_data['label_0'].append(item['label_0'])
        hf_data['caption'].append(item['caption'])

    # Create HuggingFace dataset
    hf_dataset = HFDataset.from_dict(hf_data)

    # Save to disk
    hf_dataset.save_to_disk(output_path)
    print(f"HuggingFace dataset saved to {output_path}")


def inspect_dataset(dataset_path: str, num_samples: int = 3):
    """
    Inspect the preference dataset and print sample information.

    Args:
        dataset_path: Path to the .pkl preference dataset
        num_samples: Number of samples to display
    """
    dataset = PreferenceDataset(dataset_path)

    print("\n" + "="*60)
    print("DATASET INSPECTION")
    print("="*60)

    # Print summary
    summary = dataset.get_metadata_summary()
    print(f"Total preference pairs: {summary['total_pairs']}")
    print(f"Primary metric used: {summary['primary_metric_used']}")

    if summary['score_difference_stats']:
        stats = summary['score_difference_stats']
        print(
            f"Score difference - Mean: {stats['mean']:.4f}, Min: {stats['min']:.4f}, Max: {stats['max']:.4f}")

    print("\n" + "-"*40)
    print(f"SAMPLE DATA (first {num_samples} pairs)")
    print("-"*40)

    for i in range(min(num_samples, len(dataset))):
        item = dataset[i]

        print(f"\nPair {i+1}:")
        print(f"  Caption: {item['caption'][:100]}..." if len(
            item['caption']) > 100 else f"  Caption: {item['caption']}")
        print(
            f"  Label: {item['label_0']} (jpg_0 is {'preferred' if item['label_0'] == 1 else 'not preferred'})")
        print(
            f"  Image sizes: jpg_0={len(item['jpg_0'])} bytes, jpg_1={len(item['jpg_1'])} bytes")

        if 'metadata' in item:
            meta = item['metadata']
            print(
                f"  Best: {meta.get('best_filename', 'unknown')} (score: {meta.get('best_scores', {}).get(meta.get('primary_metric', ''), 'N/A')})")
            print(
                f"  Worst: {meta.get('worst_filename', 'unknown')} (score: {meta.get('worst_scores', {}).get(meta.get('primary_metric', ''), 'N/A')})")
            print(f"  Score difference: {meta.get('score_difference', 'N/A')}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Inspect or convert preference dataset")
    parser.add_argument("--dataset_path", type=str, required=True,
                        help="Path to the .pkl preference dataset")
    parser.add_argument("--inspect", action="store_true",
                        help="Inspect the dataset and show samples")
    parser.add_argument("--convert_to_hf", type=str,
                        help="Convert to HuggingFace format and save to specified path")
    parser.add_argument("--num_samples", type=int, default=3,
                        help="Number of samples to show when inspecting")

    args = parser.parse_args()

    if args.inspect:
        inspect_dataset(args.dataset_path, args.num_samples)

    if args.convert_to_hf:
        convert_to_huggingface_format(args.dataset_path, args.convert_to_hf)

